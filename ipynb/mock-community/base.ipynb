{
 "metadata": {
  "name": "",
  "signature": "sha256:304b4885ca2008158c82aed9bc411992e145f9906a181f200fa3166ae2f7288b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Taxonomic assignment method evaluation framework\n",
      "================================================\n",
      "\n",
      "This [IPython Notebook](http://ipython.org/notebook.html) illustrates how to apply the evaluation framework described in (Bokulich, Rideout, et al. (in preparation)). Given a set of precomputed taxonomic assignment method evaluation results (this will likely be the ones included in the [short-read-tax-assignment GitHub repository](https://github.com/gregcaporaso/short-read-tax-assignment/)) and a set of results generated by the user for a new method, the results from the new method are analyzed in the context of the precomputed results. This allows users to rapidly determine how a new method performs, relative to the precomputed results. \n",
      "\n",
      "The following sections present different evaluations with an example usearch-based taxonomic assigner. A parameter sweep was performed using this new taxonomic assigner (as described in the [usearch-tax-assigner notebook](http://nbviewer.ipython.org/github/gregcaporaso/short-read-tax-assignment/blob/master/ipynb/0-usearch-tax-analyzer.ipynb)), and the evaluations that are run here allow us to determine how the new assigner performs relative to the precomputed results and summarize the results in a report, which will be this notebook after all cells have been executed. This provides a convenient interactive framework for analyzing new methods. \n",
      "\n",
      "This notebook could easily be applied to the results of a different taxonomic assigner, and it is therefore intended to serve as a template for report generation. Only one change would need to be made to support that, and that change is discussed inline below.\n",
      "\n",
      "This code and other components of this framework can be found in the [short-read-tax-assignment GitHub repository](https://github.com/gregcaporaso/short-read-tax-assignment/).\n",
      "\n",
      "Requirements\n",
      "------------\n",
      "\n",
      "To run this notebook, you'll need to have the following installed:\n",
      "\n",
      " * [IPython](http://ipython.org/) >= 2.0.0\n",
      " * [short-read-tax-assignment](https://github.com/gregcaporaso/short-read-tax-assignment)\n",
      " * [scikit-bio](http://scikit-bio.org) == 0.2.0\n",
      " * [biom-format](http://biom-format.org) >= 2.0.1\n",
      " \n",
      "Terminology\n",
      "-----------\n",
      "\n",
      "Throughout the notebook and associated code, we refer to the precomputed results obtained from GitHub as the *subject results*, and the results generated by the user as the *query results*.\n",
      "\n",
      "Structuring query results for comparison to subject results\n",
      "-----------------------------------------------------------\n",
      "\n",
      "To prepare results from another classifier for analysis, you'll need to have [BIOM](http://www.biom-format.org) files with taxonomy assignments as an observation metadata category called ``taxonomy``. These should be generated for all analyses (mock community, natural community), datasets, reference databases, methods, and parameter combinations of interest. An example of how to generate these is presented in the [usearch-tax-assigner notebook](http://nbviewer.ipython.org/github/gregcaporaso/short-read-tax-assignment/blob/master/ipynb/0-usearch-tax-analyzer.ipynb)).\n",
      "\n",
      "Your BIOM tables should be called ``table.biom``, and nested in the following directory structure:\n",
      "\n",
      "```\n",
      "query_results_dir/\n",
      " analysis/\n",
      "  dataset-id/ \n",
      "   reference-db-id/\n",
      "    method-id/\n",
      "     parameter-combination-id/\n",
      "      table.biom\n",
      "```\n",
      "\n",
      "``query_results_dir`` is the name of the top level directory, and you will set this value in the first code cell of this notebook. You can name this directory whatever you want to.\n",
      "\n",
      "This directory structure is identical to that for the [subject results](https://github.com/gregcaporaso/short-read-tax-assignment/tree/master/data/eval-subject-results). You can review that directory structure for an example of how this should look.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Configure local environment-specific values\n",
      "-------------------------------------------\n",
      "\n",
      "**This is the only cell that you will need to edit to generate reports locally.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## short_read_tax_dir should be the directory where you've downloaded (or cloned) the \n",
      "## short-read-tax-assignment repository. \n",
      "short_read_tax_dir = \"/Users/caporaso/Dropbox/code/short-read-tax-assignment/\"\n",
      "\n",
      "## query_results_dirs should contain the directory or directories where the results \n",
      "## of the method(s) to be compared to the pre-computed results can be found. \n",
      "query_results_dirs = \\\n",
      " [\"/Users/caporaso/analysis/2014.09.09-tax-assignment-sweep/2014.09.03-tax-parameter-sweep\"]\n",
      "\n",
      "# taxonomic level to perform the analysis at\n",
      "# 2: phylum, 3: class, 4: order, 5: family, 6: genus\n",
      "taxonomic_level = 4\n",
      "\n",
      "# minimum number of times an OTU must be observed for it to be included in analyses\n",
      "min_count = 10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Prepare the environment\n",
      "-----------------------\n",
      "\n",
      "First we'll configure IPython to add matplotlib plots inline. Then we'll import various functions that we'll need for generating the report. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "from os.path import join, exists\n",
      "import pandas as pd\n",
      "from skbio.draw import boxplots\n",
      "\n",
      "from taxcompare.eval_framework import (get_expected_tables_lookup, \n",
      "                                       find_and_process_result_tables,\n",
      "                                       compute_mock_results,\n",
      "                                       compute_mantel,\n",
      "                                       generate_pr_scatter_plots,\n",
      "                                       boxplot_from_data_frame,\n",
      "                                       heatmap_from_data_frame,\n",
      "                                       method_by_dataset_a1,\n",
      "                                       method_by_dataset_a2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define the subdirectories where the query mock community data should be, and confirm that they exist.\n",
      "mock_query_results_dirs = [join(query_results_dir,\"mock-community\") for query_results_dir in query_results_dirs]\n",
      "\n",
      "for mock_query_results_dir in mock_query_results_dirs:\n",
      "    assert exists(mock_query_results_dir), \"Mock community query directory doesn't exist: %s\" % mock_query_results_dir\n",
      "\n",
      "# Define the subdirectories where the subject mock community data should be, and confirm that they exist.\n",
      "mock_subject_results_dir = join(short_read_tax_dir,'data/subject-results/mock-community/')\n",
      "\n",
      "assert exists(mock_subject_results_dir), \"Mock community subject directory doesn't exist: %s\" % mock_subject_results_dir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Find mock community pre-computed tables, expected tables, and \"query\" tables\n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "Next we'll use the paths defined above to find all of the tables that will be compared. These include the *pre-computed result* tables (i.e., the ones that the new methods will be compared to), the *expected result* tables (i.e., the tables containing the known composition of the mock microbial communities), and the *query result* tables (i.e., the tables generated with the new method(s) that we want to compare to the *pre-computed result* tables)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "query_results = []\n",
      "for mock_query_results_dir in mock_query_results_dirs:\n",
      "    query_results += find_and_process_result_tables(mock_query_results_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subject_results = find_and_process_result_tables(mock_subject_results_dir)\n",
      "expected_tables = get_expected_tables_lookup(mock_subject_results_dir, level=taxonomic_level)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Uncomment for test runs (looks at a small subset of the data)\n",
      "\n",
      "# from random import shuffle\n",
      "# shuffle(subject_results)\n",
      "# shuffle(query_results)\n",
      "# subject_results = subject_results[:10]\n",
      "# query_results = query_results[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Evalution 1: Compute and summarize precision, recall, and F-measure for mock communities\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "In this evaluation, we compute and summarize precision, recall, and F-measure of each result (pre-computed and query) based on the known composition of the mock communities. We then summarize the results in two ways: first with boxplots, and second with a table of the top methods based on their F-measures. \n",
      "\n",
      "This is a qualitative evaluation, effectively telling us about the ability of the different methods to report the taxa that are present in each sample. These metrics are not concerned with the abundance of the different taxa."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subject_mock_results = compute_mock_results(subject_results, expected_tables, taxonomy_level=taxonomic_level,\n",
      "                                            min_count=min_count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "query_mock_results = compute_mock_results(query_results, expected_tables, taxonomy_level=taxonomic_level,\n",
      "                                          min_count=min_count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_mock_results = pd.concat([subject_mock_results, query_mock_results])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "boxplot_from_data_frame(all_mock_results, group_by=\"Method\", metric=\"Precision\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "boxplot_from_data_frame(all_mock_results, group_by=\"Method\", metric=\"Recall\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "boxplot_from_data_frame(all_mock_results, group_by=\"Method\", metric=\"F-measure\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "boxplot_from_data_frame(all_mock_results, group_by=\"Dataset\", metric=\"Precision\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "boxplot_from_data_frame(all_mock_results, group_by=\"Dataset\", metric=\"Recall\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "boxplot_from_data_frame(all_mock_results, group_by=\"Dataset\", metric=\"F-measure\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "heatmap_from_data_frame(all_mock_results, \"Precision\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "heatmap_from_data_frame(all_mock_results, \"Recall\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "heatmap_from_data_frame(all_mock_results, \"F-measure\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a1(all_mock_results, 'B1')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a1(all_mock_results, 'B2')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a1(all_mock_results, 'B3')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a1(all_mock_results, 'B4')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a1(all_mock_results, 'B5')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a1(all_mock_results, 'B6')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a1(all_mock_results, 'B7')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a1(all_mock_results, 'B8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a1(all_mock_results, 'F1')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a1(all_mock_results, 'F2')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Evaluation 2: Compute and summarize correlations between observed and known mock community structure\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "In this evaluation, we compute and summarize the correlation between each result (pre-computed and query) and the known composition of the mock communities. We then summarize the results in two ways: first with a series of boxplots of correlation coefficients by method; and second with a table of the top methods based on their Pearson correlation coefficient. \n",
      "\n",
      "This is a quantitative evaluation, which tells us about the ability of the different methods to report the taxa that are present in each sample and accurately assess their abundance. Because many factors can affect the observed abundance of taxa beyond the accuracy of the taxonomic assigner (e.g., primer bias), the correlation coefficients are frequently low, but we expect that their relative values are informative in understanding which taxonomic assigners are more correct than others."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "boxplot_from_data_frame(all_mock_results, group_by=\"Method\", metric=\"Pearson r\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "boxplot_from_data_frame(all_mock_results, group_by=\"Method\", metric=\"Spearman r\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "heatmap_from_data_frame(all_mock_results, \"Pearson r\", vmin=-1, vmax=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "heatmap_from_data_frame(all_mock_results, \"Spearman r\", vmin=-1, vmax=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a2(all_mock_results, 'B1')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a2(all_mock_results, 'B2')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a2(all_mock_results, 'B3')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a2(all_mock_results, 'B4')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a2(all_mock_results, 'B5')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a2(all_mock_results, 'B6')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a2(all_mock_results, 'B7')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a2(all_mock_results, 'B8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a2(all_mock_results, 'F1')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a2(all_mock_results, 'F2')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}