{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation: using Python to sweep over methods and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we illustrate how to use Python to perform *parameter sweeps* for a taxonomic assigner and integrate the results into the TAX CREdiT framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os.path import join, exists, split, sep, expandvars \n",
    "from os import makedirs, getpid\n",
    "from glob import glob\n",
    "from shutil import rmtree\n",
    "import csv\n",
    "import json\n",
    "\n",
    "from qiime2.plugins import feature_classifier\n",
    "from qiime2 import Artifact\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from q2_feature_classifier.custom import LowMemoryMultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from q2_feature_classifier.classifier import spec_from_pipeline\n",
    "from q2_types.feature_data import DNAIterator\n",
    "from pandas import DataFrame\n",
    "\n",
    "from tax_credit.framework_functions import gen_param_sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "project_dir = expandvars('$HOME/Projects/short-read-tax-assignment-bk/')\n",
    "analysis_name = 'mock-community'\n",
    "data_dir = join(project_dir, 'data', analysis_name)\n",
    "\n",
    "reference_database_dir = join(project_dir, 'data', 'ref_dbs')\n",
    "results_dir = join(project_dir, 'sandpit', analysis_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data set sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're going to define the data sets that we'll sweep over. The following cell does not need to be modified unless if you wish to change the datasets or reference databases used in the sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mock_dirs = ['mock-'+str(i) for i in range(1,11)]\n",
    "dataset_reference_combinations = \\\n",
    "    zip(mock_dirs, ['gg_13_8_otus']*8 + ['unite-97-rep-set']*2)\n",
    "\n",
    "reference_dbs = {'gg_13_8_otus' : (join(reference_database_dir, 'gg_13_8_otus/99_otus_clean.fasta'), \n",
    "                                   join(reference_database_dir, 'gg_13_8_otus/99_otu_taxonomy_clean.tsv')),\n",
    "                 'unite-97-rep-set' : (join(reference_database_dir, 'unite_20.11.2016/sh_refs_qiime_ver7_99_20.11.2016_dev_clean.fasta'), \n",
    "                                       join(reference_database_dir, 'unite_20.11.2016/sh_taxonomy_qiime_ver7_99_20.11.2016_dev_clean.tsv'))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the method/parameter combinations and generating commands\n",
    "\n",
    "Now we set the methods and method-specific parameters that we want to sweep. Modify to sweep other methods. Note how method_parameters_combinations feeds method/parameter combinations to parameter_sweep() in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_parameters_combinations = {\n",
    "              'q2-multinomialNB': {'confidence': [0.0, 0.8, 1.0],\n",
    "                                   'classify__alpha': [0.001, 0.01, 0.1]}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the pipelines\n",
    "The below pipelines are used to specify the scikit-learn classifiers that are used for assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hash_params = dict(analyzer='char_wb', n_features=8192,\n",
    "                   ngram_range=[8, 8], non_negative=True)\n",
    "nb_params = dict(alpha=0.01)\n",
    "steps = [('feat_ext', HashingVectorizer(**hash_params)),\n",
    "         ('classify', MultinomialNB(**nb_params))]\n",
    "pipelines = {'q2-multinomialNB': Pipeline(steps=steps)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Methods\n",
    "The below methods are used to load the data, prepare the data, parse the classifier and classification parameters, and fit and run the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_primers(primer_file):\n",
    "    with open(primer_file) as csvfile:\n",
    "        data = next(csv.DictReader(csvfile, delimiter='\\t'))\n",
    "        return data['LinkerPrimerSequence'], data['ReversePrimer']\n",
    "    \n",
    "def guess_read_length(seqs):\n",
    "    seqs = Artifact.load(seqs)\n",
    "    lengths = [len(s) for s in seqs.view(DNAIterator)]\n",
    "    lengths.sort()\n",
    "    return lengths[len(lengths)//2]\n",
    "\n",
    "def load_trimmed_ref_seqs(input_dir, ref_seqs):\n",
    "    primer_file = join(input_dir, 'sample-metadata.tsv')\n",
    "    fprimer, rprimer = load_primers(primer_file)\n",
    "    rep_seqs = join(input_dir, 'rep_seqs.qza')\n",
    "    length = guess_read_length(rep_seqs)\n",
    "    ref_seqs = Artifact.import_data('FeatureData[Sequence]', ref_seqs)\n",
    "    return feature_classifier.methods.extract_reads(\n",
    "                sequences=ref_seqs, length=length,\n",
    "                f_primer=fprimer, r_primer=rprimer).reads\n",
    "\n",
    "def split_params(params):\n",
    "    classifier_params = feature_classifier.methods.\\\n",
    "                        classify.signature.parameters.keys()\n",
    "    pipeline_params = {k:v for k, v in params.items()\n",
    "                        if k not in classifier_params}\n",
    "    classifier_params = {k:v for k, v in params.items() \n",
    "                         if k in classifier_params}\n",
    "    return classifier_params, pipeline_params\n",
    "\n",
    "def train_and_run_classifier(output_dir, input_dir, ref_seqs, ref_taxa, method, params):    \n",
    "    # Trim the reference seqs\n",
    "    ref_reads = load_trimmed_ref_seqs(input_dir, ref_seqs)\n",
    "    \n",
    "    # Train the classifier\n",
    "    ref_taxa = Artifact.import_data('FeatureData[Taxonomy]', ref_taxa)\n",
    "    classifier_params, pipeline_params = split_params(params)\n",
    "    pipeline = pipelines[method]\n",
    "    pipeline.set_params(**pipeline_params)\n",
    "    spec = json.dumps(spec_from_pipeline(pipeline))\n",
    "    ref_reads.save('ref_reads.qza')\n",
    "    ref_taxa.save('ref_taxa.qza')\n",
    "    with open('spec.json', 'w') as spec_out:\n",
    "        spec_out.write(spec)\n",
    "    classifier = feature_classifier.methods.fit_classifier(ref_reads, ref_taxa, spec)\n",
    "    classifier = classifier.classifier\n",
    "    \n",
    "    # Classify the sequences\n",
    "    rep_seqs = Artifact.load(join(input_dir, 'rep_seqs.qza'))\n",
    "    classification = feature_classifier.methods.classify(rep_seqs, classifier, **classifier_params)\n",
    "    classification = classification.classification\n",
    "    \n",
    "    # Save the results\n",
    "    makedirs(output_dir, exist_ok=True)\n",
    "    output_file = join(output_dir, 'rep_set_tax_assignments.txt')\n",
    "    dataframe = classification.view(DataFrame)\n",
    "    dataframe.to_csv(output_file, sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sweep = gen_param_sweep(data_dir, results_dir, reference_dbs,\n",
    "                        dataset_reference_combinations,\n",
    "                        method_parameters_combinations)\n",
    "Parallel(n_jobs=4)(delayed(train_and_run_classifier)(*p) for p in sweep);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate per-method biom tables\n",
    "\n",
    "Modify the taxonomy_glob below to point to the taxonomy assignments that were generated above. This may be necessary if filepaths were altered in the preceding cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_per_method_biom_tables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e819670e5558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtaxonomy_glob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rep_set_tax_assignments.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerate_per_method_biom_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaxonomy_glob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_per_method_biom_tables' is not defined"
     ]
    }
   ],
   "source": [
    "taxonomy_glob = join(results_dir, '*', '*', '*', '*', 'rep_set_tax_assignments.txt')\n",
    "generate_per_method_biom_tables(taxonomy_glob, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move result files to repository\n",
    "\n",
    "Add results to the short-read-taxa-assignment directory (e.g., to push these results to the repository or compare with other precomputed results in downstream analysis steps). The precomputed_results_dir path and methods_dirs glob below should not need to be changed unless if substantial changes were made to filepaths in the preceding cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precomputed_results_dir = join(project_dir, \"data\", \"precomputed-results\", analysis_name)\n",
    "method_dirs = glob(join(results_dir, '*', '*', '*', '*'))\n",
    "move_results_to_repository(method_dirs, precomputed_results_dir)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:qiime2]",
   "language": "python",
   "name": "conda-env-qiime2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
