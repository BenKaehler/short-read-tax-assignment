{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database comparisons\n",
    "The purpose of this notebook is to evaluate classification accuracy between different reference databases. Select mock community sequences are taxonomically classified using two or more different reference databases, e.g., Greengenes 13_8 [trimmed to 250 nt](./generate-tax-assignments.ipynb) and the same database [trimmed to 150 nt](./generate-tax-assignments-trimmed-dbs.ipynb). [This notebook](./generate-tax-assignments-trimmed-dbs.ipynb) can also be modified to provide taxonomic classification with any number of desired reference databases/versions. Limit the analysis to only a few mock communities and method/parameter combinations; the goal here is to compare the databases, not the methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the environment\n",
    "-----------------------\n",
    "\n",
    "First we'll import various functions that we'll need for generating the report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from os import environ\n",
    "from os.path import join, exists, expandvars\n",
    "import pandas as pd\n",
    "\n",
    "from tax_credit.eval_framework import (get_expected_tables_lookup, \n",
    "                                       find_and_process_result_tables,\n",
    "                                       compute_mock_results,\n",
    "                                       compute_mantel,\n",
    "                                       generate_pr_scatter_plots,\n",
    "                                       boxplot_from_data_frame,\n",
    "                                       heatmap_from_data_frame,\n",
    "                                       method_by_dataset_a1,\n",
    "                                       method_by_dataset_a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure local environment-specific values\n",
    "-------------------------------------------\n",
    "\n",
    "**This is the only cell that you will need to edit to generate reports locally.** After editing this cell, you can run all cells in this notebook to generate your analysis report. Some of the analyses make take a few minutes to run, and analyses at more specific taxonomic levels (e.g., genus or species) will be slower than analyses at more general taxonomic levels (e.g., phylum, class). \n",
    "\n",
    "**This cell will not run until you fill in a taxonomic level (``2`` through ``7``).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## project_dir should be the directory where you've downloaded (or cloned) the \n",
    "## short-read-tax-assignment repository. \n",
    "project_dir = expandvars(\"$HOME/Desktop/projects/short-read-tax-assignment\")\n",
    "precomputed_results_dir = join(project_dir, \"data/precomputed-results/\")\n",
    "expected_results_dir = join(precomputed_results_dir, \"mock-community\")\n",
    "\n",
    "## results_dirs should contain the directory or directories where\n",
    "## results can be found. By default, this is just the precomputed \n",
    "## results included with the project. If other results should be included, \n",
    "## absolute paths to those directories should be added to this list.\n",
    "results_dirs = \\\n",
    " [precomputed_results_dir,\n",
    "  ]\n",
    "\n",
    "## Taxonomic level at which analyses should be performed. Edit this to\n",
    "## the desired taxonomic level. \n",
    "# 2: phylum, 3: class, 4: order, 5: family, 6: genus, 7: species\n",
    "taxonomic_level = 6\n",
    "\n",
    "## Minimum number of times an OTU must be observed for it to be included in analyses. Edit this\n",
    "## to analyze the effect of the minimum count on taxonomic results.\n",
    "min_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the subdirectories where the query mock community data should be, and confirm that they exist.\n",
    "mock_results_dirs = [join(results_dir,\"mock-community\") for results_dir in results_dirs]\n",
    "\n",
    "for mock_results_dir in mock_results_dirs:\n",
    "    assert exists(mock_results_dir), \"Mock community result directory doesn't exist: %s\" % mock_results_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find mock community pre-computed tables, expected tables, and \"query\" tables\n",
    "----------------------------------------------------------------------------\n",
    "\n",
    "Next we'll use the paths defined above to find all of the tables that will be compared. These include the *pre-computed result* tables (i.e., the ones that the new methods will be compared to), the *expected result* tables (i.e., the tables containing the known composition of the mock microbial communities), and the *query result* tables (i.e., the tables generated with the new method(s) that we want to compare to the *pre-computed result* tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for mock_results_dir in mock_results_dirs:\n",
    "    results += find_and_process_result_tables(mock_results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expected_tables = get_expected_tables_lookup(expected_results_dir, level=taxonomic_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment for test runs (looks at a small subset of the data)\n",
    "\n",
    "# from random import shuffle\n",
    "# shuffle(results)\n",
    "# results = results[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalution 1: Compute and summarize precision, recall, and F-measure for mock communities\n",
    "----------------------------------------------------------------------------------------\n",
    "\n",
    "In this evaluation, we compute and summarize precision, recall, and F-measure of each result (pre-computed and query) based on the known composition of the mock communities. We then summarize the results in two ways: first with boxplots, and second with a table of the top methods based on their F-measures. \n",
    "\n",
    "This is a qualitative evaluation, effectively telling us about the ability of the different methods to report the taxa that are present in each sample. These metrics are not concerned with the abundance of the different taxa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Actual table is empty after filtering at:                              /Users/nbokulich/Desktop/projects/short-read-tax-assignment/data/precomputed-results/mock-community/mock-3/gg_13_8_otus/rdp/0.0/table.biom",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-4376b5a72d11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmock_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_mock_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_tables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaxonomy_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtaxonomic_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/nbokulich/Desktop/projects/short-read-tax-assignment/tax_credit/eval_framework.py\u001b[0m in \u001b[0;36mcompute_mock_results\u001b[0;34m(result_tables, expected_table_lookup, taxonomy_level, min_count, taxa_to_keep, new_param_ids)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mactual_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             raise ValueError(\"Actual table is empty after filtering at: \\\n\u001b[0;32m--> 418\u001b[0;31m                              {0}\".format(actual_table_fp))\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mcollapse_by_taxonomy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_taxonomy_collapser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaxonomy_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Actual table is empty after filtering at:                              /Users/nbokulich/Desktop/projects/short-read-tax-assignment/data/precomputed-results/mock-community/mock-3/gg_13_8_otus/rdp/0.0/table.biom"
     ]
    }
   ],
   "source": [
    "mock_results = compute_mock_results(results, expected_tables, taxonomy_level=taxonomic_level, min_count=min_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplot_from_data_frame(mock_results, group_by=\"Method\", metric=\"Precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplot_from_data_frame(mock_results, group_by=\"Method\", metric=\"Recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplot_from_data_frame(mock_results, group_by=\"Method\", metric=\"F-measure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplot_from_data_frame(mock_results, group_by=\"Dataset\", metric=\"Precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplot_from_data_frame(mock_results, group_by=\"Dataset\", metric=\"Recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplot_from_data_frame(mock_results, group_by=\"Dataset\", metric=\"F-measure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heatmap_from_data_frame(mock_results, \"Precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heatmap_from_data_frame(mock_results, \"Recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heatmap_from_data_frame(mock_results, \"F-measure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a1(mock_results, 'B1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a1(mock_results, 'B2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a1(mock_results, 'B3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a1(mock_results, 'B4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a1(mock_results, 'B5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a1(mock_results, 'B6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a1(mock_results, 'B7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a1(mock_results, 'B8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a1(mock_results, 'F1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a1(mock_results, 'F2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation 2: Compute and summarize correlations between observed and known mock community structure\n",
    "----------------------------------------------------------------------------------------------------\n",
    "\n",
    "In this evaluation, we compute and summarize the correlation between each result (pre-computed and query) and the known composition of the mock communities. We then summarize the results in two ways: first with a series of boxplots of correlation coefficients by method; and second with a table of the top methods based on their Pearson correlation coefficient. \n",
    "\n",
    "This is a quantitative evaluation, which tells us about the ability of the different methods to report the taxa that are present in each sample and accurately assess their abundance. Because many factors can affect the observed abundance of taxa beyond the accuracy of the taxonomic assigner (e.g., primer bias), the correlation coefficients are frequently low, but we expect that their relative values are informative in understanding which taxonomic assigners are more correct than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplot_from_data_frame(mock_results, group_by=\"Method\", metric=\"Pearson r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplot_from_data_frame(mock_results, group_by=\"Method\", metric=\"Spearman r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heatmap_from_data_frame(mock_results, \"Pearson r\", vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heatmap_from_data_frame(mock_results, \"Spearman r\", vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a2(mock_results, 'B1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a2(mock_results, 'B2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a2(mock_results, 'B3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a2(mock_results, 'B4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a2(mock_results, 'B5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a2(mock_results, 'B6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a2(mock_results, 'B7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a2(mock_results, 'B8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a2(mock_results, 'F1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a2(mock_results, 'F2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
