{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the environment\n",
    "-----------------------\n",
    "\n",
    "First we'll import various functions that we'll need for generating the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from os.path import join, exists, expandvars\n",
    "import pandas as pd\n",
    "from skbio.draw import boxplots\n",
    "\n",
    "from taxcompare.eval_framework import (get_expected_tables_lookup, \n",
    "                                       find_and_process_result_tables,\n",
    "                                       compute_mock_results,\n",
    "                                       compute_mantel,\n",
    "                                       generate_pr_scatter_plots,\n",
    "                                       boxplot_from_data_frame,\n",
    "                                       heatmap_from_data_frame,\n",
    "                                       method_by_dataset_a1,\n",
    "                                       method_by_dataset_a2,\n",
    "                                       performance_rank_comparisons,\n",
    "                                       parameter_comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure local environment-specific values\n",
    "-------------------------------------------\n",
    "\n",
    "**This is the only cell that you will need to edit to generate reports locally.** After editing this cell, you can run all cells in this notebook to generate your analysis report. Some of the analyses make take a few minutes to run, and analyses at more specific taxonomic levels (e.g., genus or species) will be slower than analyses at more general taxonomic levels (e.g., phylum, class). \n",
    "\n",
    "**This cell will not run until you fill in a taxonomic level (``2`` through ``7``).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## project_dir should be the directory where you've downloaded (or cloned) the \n",
    "## short-read-tax-assignment repository. \n",
    "project_dir = expandvars(\"$HOME/data/short-read-tax-assignment\")\n",
    "\n",
    "precomputed_results_dir = expandvars(\"$HOME/data/short-read-tax-assignment/data/precomputed-results/\")\n",
    "expected_results_dir = join(precomputed_results_dir, \"simulated-community\")\n",
    "\n",
    "## results_dirs should contain the directory or directories where\n",
    "## results can be found. By default, this is just the precomputed \n",
    "## results included with the project. If other results should be included, \n",
    "## absolute paths to those directories should be added to this list.\n",
    "results_dirs = \\\n",
    " [precomputed_results_dir,\n",
    "  expandvars(\"$HOME/data/2015.06.24-tax-parameter-sweep-simulated\")\n",
    "  ]\n",
    "\n",
    "    \n",
    "new_param_ids = {'mothur':['confidence']}\n",
    "## Taxonomic level at which analyses should be performed. Edit this to\n",
    "## the desired taxonomic level. \n",
    "# 2: phylum, 3: class, 4: order, 5: family, 6: genus, 7: species\n",
    "taxonomic_level = 2\n",
    "\n",
    "## Reference choice (must be partial-ref or full-ref)\n",
    "reference_choice = \"partial-ref\"\n",
    "\n",
    "## Minimum number of times an OTU must be observed for it to be included in analyses. Edit this\n",
    "## to analyze the effect of the minimum count on taxonomic results.\n",
    "min_count = 1\n",
    "\n",
    "# set to true if select tables should be written as Excel files (useful for publication)\n",
    "write_xls_files = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the subdirectories where the data should be, and confirm that they exist.\n",
    "simulated_results_dirs = [join(results_dir,\"simulated-community\") for results_dir in results_dirs]\n",
    "\n",
    "for simulated_results_dir in simulated_results_dirs:\n",
    "    assert exists(simulated_results_dir), \"Simulated community result directory doesn't exist: %s\" % simulated_results_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find pre-computed tables, expected tables, and \"query\" tables\n",
    "-------------------------------------------------------------\n",
    "\n",
    "Next we'll use the paths defined above to find all of the tables that will be compared. These include the *pre-computed result* tables (i.e., the ones that the new methods will be compared to), the *expected result* tables (i.e., the tables containing the known composition of the mock microbial communities), and the *query result* tables (i.e., the tables generated with the new method(s) that we want to compare to the *pre-computed result* tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for simulated_results_dir in simulated_results_dirs:\n",
    "    results += find_and_process_result_tables(simulated_results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment for test runs (looks at a small subset of the data)\n",
    "\n",
    "# from random import shuffle\n",
    "# shuffle(results)\n",
    "# results = results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_fp = join(precomputed_results_dir,'simulated-community', 'level%d-results-w-mothur.csv') % taxonomic_level\n",
    "\n",
    "if exists(result_fp):\n",
    "    simulated_results = pd.DataFrame.from_csv(result_fp)\n",
    "else:\n",
    "    expected_tables = get_expected_tables_lookup(expected_results_dir, level=taxonomic_level)\n",
    "    simulated_results = compute_mock_results(results, expected_tables, taxonomy_level=taxonomic_level, min_count=min_count, new_param_ids=new_param_ids)\n",
    "    simulated_results.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "refernece_choice_v = [e.endswith(reference_choice) for e in simulated_results['Parameters']]\n",
    "simulated_results = simulated_results[refernece_choice_v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalution 1: Compute and summarize precision, recall, and F-measure\n",
    "-------------------------------------------------------------------\n",
    "\n",
    "In this evaluation, we compute and summarize precision, recall, and F-measure of each result (pre-computed and query) based on the known composition of the simulated communities. We then summarize the results in two ways: first with boxplots, and second with a table of the top methods based on their F-measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplot_from_data_frame(simulated_results, group_by=\"Method\", metric=\"Precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplot_from_data_frame(simulated_results, group_by=\"Method\", metric=\"Recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplot_from_data_frame(simulated_results, group_by=\"Method\", metric=\"F-measure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplot_from_data_frame(simulated_results, group_by=\"Dataset\", metric=\"Precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplot_from_data_frame(simulated_results, group_by=\"Dataset\", metric=\"Recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplot_from_data_frame(simulated_results, group_by=\"Dataset\", metric=\"F-measure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heatmap_from_data_frame(simulated_results, \"Precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heatmap_from_data_frame(simulated_results, \"Recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heatmap_from_data_frame(simulated_results, \"F-measure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a1(simulated_results, 'B1-iter0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a1(simulated_results, 'B2-iter0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a1(simulated_results, 'F1-iter0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_by_dataset_a1(simulated_results, 'F2-iter0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation 2: Rank-based statistics comparing the performance of the optimal parameter setting run for each method on each data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Count best* column indicates how many samples a given method achieved the best result or tied for the best result (which is why they sum to more than the total number of samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Within-method comparisons of parameter performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdp_top_params = parameter_comparisons(simulated_results, \"rdp\", metrics=['Precision', 'Recall', 'F-measure'])\n",
    "rdp_top_params[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uclust_top_params = parameter_comparisons(simulated_results, \"uclust\", metrics=['Precision', 'Recall', 'F-measure'])\n",
    "uclust_top_params[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sortmerna_top_params = parameter_comparisons(simulated_results, \"sortmerna\", metrics=['Precision', 'Recall', 'F-measure'])\n",
    "sortmerna_top_params[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blast_top_params = parameter_comparisons(simulated_results, \"blast\", metrics=['Precision', 'Recall', 'F-measure'])\n",
    "blast_top_params[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mothur_top_params = parameter_comparisons(simulated_results, \"mothur\", metrics=['Precision', 'Recall', 'F-measure'])\n",
    "mothur_top_params[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Between-method performance comparisons based on best parameter set determined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mp_combs = {\"rdp\": rdp_top_params.index[0],\n",
    "            \"blast\": blast_top_params.index[0], \n",
    "            \"sortmerna\": sortmerna_top_params.index[0],\n",
    "            \"uclust\": uclust_top_params.index[0],\n",
    "            \"mothur\": mothur_top_params.index[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric = 'Precision'\n",
    "df = performance_rank_comparisons(simulated_results, metric, mp_combs)\n",
    "if write_xls_files: \n",
    "    df.to_excel('tables/level%d_%s_rankstats.xlsx' % (taxonomic_level, metric), \n",
    "                                      na_rep='NA', float_format=\"%1.3f\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric = 'Recall'\n",
    "df = performance_rank_comparisons(simulated_results, metric, mp_combs)\n",
    "if write_xls_files: \n",
    "    df.to_excel('tables/level%d_%s_rankstats.xlsx' % (taxonomic_level, metric), \n",
    "                                      na_rep='NA', float_format=\"%1.3f\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric = 'F-measure'\n",
    "df = performance_rank_comparisons(simulated_results, metric, mp_combs)\n",
    "if write_xls_files: \n",
    "    df.to_excel('tables/level%d_%s_rankstats.xlsx' % (taxonomic_level, metric), \n",
    "                                      na_rep='NA', float_format=\"%1.3f\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
