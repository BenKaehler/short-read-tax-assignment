{
 "metadata": {
  "name": "",
  "signature": "sha256:fb176b8982b054448dc5d51870ccffeed6a97598007449694ab9364857e934f4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Prepare the environment\n",
      "-----------------------\n",
      "\n",
      "First we'll import various functions that we'll need for generating the report."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "from os.path import join, exists, expandvars\n",
      "import pandas as pd\n",
      "from skbio.draw import boxplots\n",
      "\n",
      "from taxcompare.eval_framework import (get_expected_tables_lookup, \n",
      "                                       find_and_process_result_tables,\n",
      "                                       compute_mock_results,\n",
      "                                       compute_mantel,\n",
      "                                       generate_pr_scatter_plots,\n",
      "                                       boxplot_from_data_frame,\n",
      "                                       heatmap_from_data_frame,\n",
      "                                       method_by_dataset_a1,\n",
      "                                       method_by_dataset_a2,\n",
      "                                       performance_rank_comparisons,\n",
      "                                       parameter_comparisons)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Configure local environment-specific values\n",
      "-------------------------------------------\n",
      "\n",
      "**This is the only cell that you will need to edit to generate reports locally.** After editing this cell, you can run all cells in this notebook to generate your analysis report. Some of the analyses make take a few minutes to run, and analyses at more specific taxonomic levels (e.g., genus or species) will be slower than analyses at more general taxonomic levels (e.g., phylum, class). \n",
      "\n",
      "**This cell will not run until you fill in a taxonomic level (``2`` through ``7``).**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## project_dir should be the directory where you've downloaded (or cloned) the \n",
      "## short-read-tax-assignment repository. \n",
      "project_dir = expandvars(\"$HOME/data/short-read-tax-assignment\")\n",
      "\n",
      "precomputed_results_dir = expandvars(\"$HOME/data/short-read-tax-assignment/data/precomputed-results/\")\n",
      "expected_results_dir = join(precomputed_results_dir, \"simulated-community\")\n",
      "\n",
      "## results_dirs should contain the directory or directories where\n",
      "## results can be found. By default, this is just the precomputed \n",
      "## results included with the project. If other results should be included, \n",
      "## absolute paths to those directories should be added to this list.\n",
      "results_dirs = \\\n",
      " [precomputed_results_dir,\n",
      "  ]\n",
      "\n",
      "## Taxonomic level at which analyses should be performed. Edit this to\n",
      "## the desired taxonomic level. \n",
      "# 2: phylum, 3: class, 4: order, 5: family, 6: genus, 7: species\n",
      "taxonomic_level = ENTER TAXONOMIC LEVEL HERE\n",
      "\n",
      "## Reference choice (must be partial-ref or full-ref)\n",
      "reference_choice = ENTER REFERENCE CHOICE HERE\n",
      "\n",
      "## Minimum number of times an OTU must be observed for it to be included in analyses. Edit this\n",
      "## to analyze the effect of the minimum count on taxonomic results.\n",
      "min_count = 1\n",
      "\n",
      "# set to true if select tables should be written as Excel files (useful for publication)\n",
      "write_xls_files = False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define the subdirectories where the data should be, and confirm that they exist.\n",
      "simulated_results_dirs = [join(results_dir,\"simulated-community\") for results_dir in results_dirs]\n",
      "\n",
      "for simulated_results_dir in simulated_results_dirs:\n",
      "    assert exists(simulated_results_dir), \"Simulated community result directory doesn't exist: %s\" % simulated_results_dir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Find pre-computed tables, expected tables, and \"query\" tables\n",
      "-------------------------------------------------------------\n",
      "\n",
      "Next we'll use the paths defined above to find all of the tables that will be compared. These include the *pre-computed result* tables (i.e., the ones that the new methods will be compared to), the *expected result* tables (i.e., the tables containing the known composition of the mock microbial communities), and the *query result* tables (i.e., the tables generated with the new method(s) that we want to compare to the *pre-computed result* tables)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = []\n",
      "for simulated_results_dir in simulated_results_dirs:\n",
      "    results += find_and_process_result_tables(simulated_results_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Uncomment for test runs (looks at a small subset of the data)\n",
      "\n",
      "# from random import shuffle\n",
      "# shuffle(results)\n",
      "# results = results[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result_fp = join(precomputed_results_dir,'simulated-community', 'level%d-results.csv') % taxonomic_level\n",
      "if exists(result_fp):\n",
      "    simulated_results = pd.DataFrame.from_csv(result_fp)\n",
      "else:\n",
      "    expected_tables = get_expected_tables_lookup(expected_results_dir, level=taxonomic_level)\n",
      "    simulated_results = compute_mock_results(results, expected_tables, taxonomy_level=taxonomic_level, min_count=min_count)\n",
      "    simulated_results.to_csv()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "refernece_choice_v = [e.endswith(reference_choice) for e in simulated_results['Parameters']]\n",
      "simulated_results = simulated_results[refernece_choice_v]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Evalution 1: Compute and summarize precision, recall, and F-measure\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "In this evaluation, we compute and summarize precision, recall, and F-measure of each result (pre-computed and query) based on the known composition of the simulated communities. We then summarize the results in two ways: first with boxplots, and second with a table of the top methods based on their F-measures."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "boxplot_from_data_frame(simulated_results, group_by=\"Method\", metric=\"Precision\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "boxplot_from_data_frame(simulated_results, group_by=\"Method\", metric=\"Recall\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "boxplot_from_data_frame(simulated_results, group_by=\"Method\", metric=\"F-measure\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "boxplot_from_data_frame(simulated_results, group_by=\"Dataset\", metric=\"Precision\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "boxplot_from_data_frame(simulated_results, group_by=\"Dataset\", metric=\"Recall\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "boxplot_from_data_frame(simulated_results, group_by=\"Dataset\", metric=\"F-measure\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "heatmap_from_data_frame(simulated_results, \"Precision\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "heatmap_from_data_frame(simulated_results, \"Recall\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "heatmap_from_data_frame(simulated_results, \"F-measure\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a1(simulated_results, 'B1-iter0')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a1(simulated_results, 'B2-iter0')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a1(simulated_results, 'F1-iter0')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "method_by_dataset_a1(simulated_results, 'F2-iter0')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Evaluation 2: Rank-based statistics comparing the performance of the optimal parameter setting run for each method on each data set."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Count best* column indicates how many samples a given method achieved the best result or tied for the best result (which is why they sum to more than the total number of samples)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Within-method comparisons of parameter performance"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdp_top_params = parameter_comparisons(simulated_results, \"rdp\", metrics=['Precision', 'Recall', 'F-measure'])\n",
      "rdp_top_params[:15]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "uclust_top_params = parameter_comparisons(simulated_results, \"uclust\", metrics=['Precision', 'Recall', 'F-measure'])\n",
      "uclust_top_params[:15]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sortmerna_top_params = parameter_comparisons(simulated_results, \"sortmerna\", metrics=['Precision', 'Recall', 'F-measure'])\n",
      "sortmerna_top_params[:15]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blast_top_params = parameter_comparisons(simulated_results, \"blast\", metrics=['Precision', 'Recall', 'F-measure'])\n",
      "blast_top_params[:15]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Between-method performance comparisons based on best parameter set determined above"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mp_combs = {\"rdp\": rdp_top_params.index[0],\n",
      "            \"blast\": blast_top_params.index[0], \n",
      "            \"sortmerna\": sortmerna_top_params.index[0],\n",
      "            \"uclust\": uclust_top_params.index[0]}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metric = 'Precision'\n",
      "df = performance_rank_comparisons(simulated_results, metric, mp_combs)\n",
      "if write_xls_files: \n",
      "    df.to_excel('tables/level%d_%s_rankstats.xlsx' % (taxonomic_level, metric), \n",
      "                                      na_rep='NA', float_format=\"%1.3f\")\n",
      "df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metric = 'Recall'\n",
      "df = performance_rank_comparisons(simulated_results, metric, mp_combs)\n",
      "if write_xls_files: \n",
      "    df.to_excel('tables/level%d_%s_rankstats.xlsx' % (taxonomic_level, metric), \n",
      "                                      na_rep='NA', float_format=\"%1.3f\")\n",
      "df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metric = 'F-measure'\n",
      "df = performance_rank_comparisons(simulated_results, metric, mp_combs)\n",
      "if write_xls_files: \n",
      "    df.to_excel('tables/level%d_%s_rankstats.xlsx' % (taxonomic_level, metric), \n",
      "                                      na_rep='NA', float_format=\"%1.3f\")\n",
      "df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}