{
 "metadata": {
  "name": "",
  "signature": "sha256:b0ee4b794ac6b7014fab3a3cef9989ad82982748859788a5a6760a33d7b12a13"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Prepare the environment"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First we'll configure IPython to add matplotlib plots inline. Then we'll import various functions that we'll need for generating the report. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "from os.path import join, exists, expandvars\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from seaborn import heatmap\n",
      "from seaborn import clustermap\n",
      "import matplotlib.pyplot as plt\n",
      "from taxcompare.eval_framework import (get_expected_tables_lookup, \n",
      "                                       find_and_process_result_tables,\n",
      "                                       compute_mock_results,\n",
      "                                       compute_mantel,\n",
      "                                       generate_pr_scatter_plots,\n",
      "                                       boxplot_from_data_frame,\n",
      "                                       heatmap_from_data_frame,\n",
      "                                       method_by_dataset_a1,\n",
      "                                       method_by_dataset_a2,\n",
      "                                       performance_rank_comparisons,\n",
      "                                       parameter_comparisons)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Configure local environment-specific values"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**This is the only cell that you will need to edit to generate reports locally.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## project_dir should be the directory where you've downloaded (or cloned) the \n",
      "## short-read-tax-assignment repository. \n",
      "project_dir = expandvars(\"$HOME/data/short-read-tax-assignment\")\n",
      "\n",
      "precomputed_results_dir = expandvars(\"$HOME/data/short-read-tax-assignment/data/precomputed-results/\")\n",
      "expected_results_dir = join(precomputed_results_dir, \"simulated-community\")\n",
      "\n",
      "## results_dirs should contain the directory or directories where\n",
      "## results can be found. By default, this is just the precomputed \n",
      "## results included with the project. If other results should be included, \n",
      "## absolute paths to those directories should be added to this list.\n",
      "results_dirs = \\\n",
      " [precomputed_results_dir,\n",
      "  ]\n",
      "\n",
      "## Taxonomic level at which analyses should be performed. Edit this to\n",
      "## the desired taxonomic level. \n",
      "# 2: phylum, 3: class, 4: order, 5: family, 6: genus, 7: species\n",
      "taxonomic_level = FILL IN VALUE HERE\n",
      "\n",
      "## Minimum number of times an OTU must be observed for it to be included in analyses. Edit this\n",
      "## to analyze the effect of the minimum count on taxonomic results.\n",
      "min_count = 1\n",
      "\n",
      "# set to true if select tables should be written as Excel files (useful for publication)\n",
      "write_xls_files = True\n",
      "\n",
      "taxonomy_level_names = ['Kingdom', 'Phylum', 'Class', 'Order', 'Family', 'Genus', 'Species']\n",
      "bacterial_reference_taxonomy_fp = expandvars(\"$HOME/data/gg_13_8_otus/taxonomy/97_otu_taxonomy.txt\")\n",
      "bacterial_reference_taxa = set()\n",
      "for line in open(bacterial_reference_taxonomy_fp, 'U'):\n",
      "    fields = line.strip().split('\\t')\n",
      "    bacterial_reference_taxa.add(tuple(fields[1].split('; ')[:taxonomic_level]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Uncomment for test runs (looks at a small subset of the data)\n",
      "\n",
      "# bacterial_reference_taxa = set(list(bacterial_reference_taxa)[:25])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define the subdirectories where the data should be, and confirm that they exist.\n",
      "simulated_results_dirs = [join(results_dir,\"simulated-community\") for results_dir in results_dirs]\n",
      "\n",
      "for simulated_results_dir in simulated_results_dirs:\n",
      "    assert exists(simulated_results_dir), \"Simulated community result directory doesn't exist: %s\" % simulated_results_dir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Find simulated community pre-computed tables, expected tables, and \"query\" tables"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we'll use the paths defined above to find all of the tables that will be compared. These include the *pre-computed result* tables (i.e., the ones that the new methods will be compared to), the *expected result* tables (i.e., the tables containing the known composition of the mock microbial communities), and the *query result* tables (i.e., the tables generated with the new method(s) that we want to compare to the *pre-computed result* tables)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = []\n",
      "for simulated_results_dir in simulated_results_dirs:\n",
      "    results += find_and_process_result_tables(simulated_results_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "expected_tables = get_expected_tables_lookup(expected_results_dir, level=taxonomic_level)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Filter to only tables that we're interested in here. These are the tables corresponding\n",
      "# to the best performing methods.\n",
      "tables_of_interest = set([('B1-iter0', 'gg_13_8_otus', 'sortmerna', '0.51:0.8:1:0.8:0.001'),\n",
      "                          ('B1-iter0', 'gg_13_8_otus', 'rdp', '0.5'),\n",
      "                          ('B1-iter0', 'gg_13_8_otus', 'uclust', '0.51:0.8:1'),\n",
      "                          ('B1-iter0', 'gg_13_8_otus', 'blast', '0.001'),\n",
      "                          ])\n",
      "\n",
      "_results = []\n",
      "for e in results:\n",
      "    if e[0:4] in tables_of_interest:\n",
      "        _results.append(e)\n",
      "results = _results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Evalution 1: Compute and summarize precision, recall, and F-measure for mock communities\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "In this evaluation, we compute and summarize precision, recall, and F-measure of each result (pre-computed and query) based on the known composition of the mock communities. We then summarize the results in two ways: first with boxplots, and second with a table of the top methods based on their F-measures. \n",
      "\n",
      "This is a qualitative evaluation, effectively telling us about the ability of the different methods to report the taxa that are present in each sample. These metrics are not concerned with the abundance of the different taxa."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "taxa_of_interest = bacterial_reference_taxa\n",
      "\n",
      "taxa_specific_results = []\n",
      "\n",
      "for taxon_of_interest in taxa_of_interest:\n",
      "    try:\n",
      "        query_mock_results = compute_mock_results(query_results, expected_tables, taxonomy_level=taxonomic_level,\n",
      "                                      min_count=min_count, taxa_to_keep=taxon_of_interest)\n",
      "    except ValueError:\n",
      "        continue\n",
      "#     try:\n",
      "#         subject_mock_results = compute_mock_results(subject_results, expected_tables, taxonomy_level=taxonomic_level,\n",
      "#                                       min_count=min_count, taxa_to_keep=taxon_of_interest)\n",
      "#     except ValueError:\n",
      "#         continue\n",
      "    taxa_specific_results.append(query_mock_results)\n",
      "        \n",
      "        \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "indices = []\n",
      "data = []\n",
      "for taxon, result in zip(bacterial_reference_taxa, taxa_specific_results):\n",
      "    values = result['F-measure']\n",
      "    methods = result['Method']\n",
      "    indices.append(list(taxon))\n",
      "    data.append(list(values))\n",
      "index = pd.MultiIndex.from_tuples(indices, names=taxonomy_level_names[:taxonomic_level])\n",
      "metric_by_taxon = pd.DataFrame(np.array(data), columns=list(methods), index=index)\n",
      "metric_by_taxon.sort_index(axis=0, inplace=True)\n",
      "\n",
      "# testing...\n",
      "# metric_by_taxon['rdp']['k__Archaea', 'p__Nanoarchaeota', 'c__[Nanoarchaeoti]'] = 1.0\n",
      "# metric_by_taxon['uclust']['k__Archaea', 'p__Nanoarchaeota', 'c__[Nanoarchaeoti]'] = 1.0\n",
      "# metric_by_taxon['sortmerna']['k__Archaea', 'p__Nanoarchaeota', 'c__[Nanoarchaeoti]'] = 1.0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metric_by_taxon"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Are method performances correlated? In other words, do all methods do well/bad on the same taxa?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print metric_by_taxon.corr()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What is the performance (F-measure, here) by taxa? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def collpase_zero_variance_taxa(df):\n",
      "\n",
      "    index_names = df.index.names\n",
      "    column_names = df.columns\n",
      "    result = df.copy()\n",
      "    for i in range(len(index_names)):\n",
      "        new_indices = []\n",
      "        new_data = []\n",
      "        for d, g in result.groupby(level=range(i+1)):\n",
      "            if len(g) == 1:\n",
      "                new_indices.append(g.index[0])\n",
      "                new_data.append(g.get_values()[0])\n",
      "            elif not g.var().any():\n",
      "                if isinstance(d, tuple):\n",
      "                    n = list(d)\n",
      "                else:\n",
      "                    n = [d]\n",
      "                n += [\"All\"] * (len(index_names) - len(n))\n",
      "                new_indices.append(n)\n",
      "                new_data.append(g.get_values()[0])\n",
      "            else:\n",
      "                new_indices.extend(g.index)\n",
      "                new_data.extend(g.get_values())\n",
      "        new_index = pd.MultiIndex.from_tuples(new_indices, names=index_names)\n",
      "        result = pd.DataFrame(new_data, columns=column_names, index=new_index)\n",
      "    return result\n",
      "\n",
      "metric_by_collapsed_taxon = collpase_zero_variance_taxa(metric_by_taxon)\n",
      "metric_by_collapsed_taxon"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Grouped by taxonomy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "height = len(metric_by_collapsed_taxon.index) * 0.35\n",
      "width = len(metric_by_collapsed_taxon.columns) * 1\n",
      "\n",
      "metric_by_collapsed_taxon.sort_index(inplace=True)\n",
      "\n",
      "# Based on SO answer: http://stackoverflow.com/a/18238680\n",
      "fig = plt.figure(figsize=(width, height))\n",
      "heatmap(metric_by_collapsed_taxon, vmin=0, vmax=1, cmap='Reds', annot=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Grouped by similarity in performance of taxonomic classifiers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#x = metric_by_collapsed_taxon[(metric_by_collapsed_taxon.mean(axis=1) < 1) & (metric_by_collapsed_taxon.mean(axis=1) > -1)]\n",
      "height = len(metric_by_collapsed_taxon.index) * 0.35\n",
      "width = len(metric_by_collapsed_taxon.columns) * 1\n",
      "#fig = plt.figure(figsize=(width, height))\n",
      "clustermap(metric_by_collapsed_taxon, vmin=0, vmax=1, cmap='Reds', figsize=(width, height), col_cluster=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for e in metric_by_collapsed_taxon.index:\n",
      "    print ' '.join(e)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}