{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll configure IPython to add matplotlib plots inline. Then we'll import various functions that we'll need for generating the report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from os.path import join, exists, expandvars\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from seaborn import heatmap\n",
    "from seaborn import clustermap\n",
    "import matplotlib.pyplot as plt\n",
    "from taxcompare.eval_framework import (get_expected_tables_lookup, \n",
    "                                       find_and_process_result_tables,\n",
    "                                       compute_mock_results,\n",
    "                                       compute_mantel,\n",
    "                                       generate_pr_scatter_plots,\n",
    "                                       boxplot_from_data_frame,\n",
    "                                       heatmap_from_data_frame,\n",
    "                                       method_by_dataset_a1,\n",
    "                                       method_by_dataset_a2,\n",
    "                                       performance_rank_comparisons,\n",
    "                                       parameter_comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure local environment-specific values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is the only cell that you will need to edit to generate reports locally.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## project_dir should be the directory where you've downloaded (or cloned) the \n",
    "## short-read-tax-assignment repository. \n",
    "project_dir = expandvars(\"$HOME/data/short-read-tax-assignment\")\n",
    "\n",
    "precomputed_results_dir = expandvars(\"$HOME/data/short-read-tax-assignment/data/precomputed-results/\")\n",
    "expected_results_dir = join(precomputed_results_dir, \"simulated-community\")\n",
    "\n",
    "## results_dirs should contain the directory or directories where\n",
    "## results can be found. By default, this is just the precomputed \n",
    "## results included with the project. If other results should be included, \n",
    "## absolute paths to those directories should be added to this list.\n",
    "results_dirs = \\\n",
    " [precomputed_results_dir,\n",
    "  ]\n",
    "\n",
    "## Taxonomic level at which analyses should be performed. Edit this to\n",
    "## the desired taxonomic level. \n",
    "# 2: phylum, 3: class, 4: order, 5: family, 6: genus, 7: species\n",
    "taxonomic_level = 6\n",
    "\n",
    "## Minimum number of times an OTU must be observed for it to be included in analyses. Edit this\n",
    "## to analyze the effect of the minimum count on taxonomic results.\n",
    "min_count = 1\n",
    "\n",
    "# set to true if select tables should be written as Excel files (useful for publication)\n",
    "write_xls_files = True\n",
    "\n",
    "taxonomy_level_names = ['Kingdom', 'Phylum', 'Class', 'Order', 'Family', 'Genus', 'Species']\n",
    "bacterial_reference_taxonomy_fp = expandvars(\"$HOME/data/gg_13_8_otus/taxonomy/97_otu_taxonomy.txt\")\n",
    "bacterial_reference_taxa = set()\n",
    "for line in open(bacterial_reference_taxonomy_fp, 'U'):\n",
    "    fields = line.strip().split('\\t')\n",
    "    bacterial_reference_taxa.add(tuple(fields[1].split('; ')[:taxonomic_level]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment for test runs (looks at a small subset of the data)\n",
    "\n",
    "# bacterial_reference_taxa = set(list(bacterial_reference_taxa)[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the subdirectories where the data should be, and confirm that they exist.\n",
    "simulated_results_dirs = [join(results_dir,\"simulated-community\") for results_dir in results_dirs]\n",
    "\n",
    "for simulated_results_dir in simulated_results_dirs:\n",
    "    assert exists(simulated_results_dir), \"Simulated community result directory doesn't exist: %s\" % simulated_results_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find simulated community pre-computed tables, expected tables, and \"query\" tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll use the paths defined above to find all of the tables that will be compared. These include the *pre-computed result* tables (i.e., the ones that the new methods will be compared to), the *expected result* tables (i.e., the tables containing the known composition of the mock microbial communities), and the *query result* tables (i.e., the tables generated with the new method(s) that we want to compare to the *pre-computed result* tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for simulated_results_dir in simulated_results_dirs:\n",
    "    results += find_and_process_result_tables(simulated_results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expected_tables = get_expected_tables_lookup(expected_results_dir, level=taxonomic_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter to only tables that we're interested in here. These are the tables corresponding\n",
    "# to the best performing methods.\n",
    "tables_of_interest = set([('B1-iter0', 'gg_13_8_otus', 'sortmerna', '0.51:0.8:1:0.8:0.001:full-ref'),\n",
    "                          ('B1-iter0', 'gg_13_8_otus', 'rdp', '0.5:full-ref'),\n",
    "                          ('B1-iter0', 'gg_13_8_otus', 'uclust', '0.51:0.8:1:full-ref'),\n",
    "                          ('B1-iter0', 'gg_13_8_otus', 'blast', '0.001:full-ref'),\n",
    "                          ])\n",
    "\n",
    "_results = []\n",
    "for e in results:\n",
    "    if e[0:4] in tables_of_interest:\n",
    "        _results.append(e)\n",
    "results = _results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B1-iter0',\n",
       "  'gg_13_8_otus',\n",
       "  'blast',\n",
       "  '0.001:full-ref',\n",
       "  '/home/ubuntu/data/short-read-tax-assignment/data/precomputed-results/simulated-community/B1-iter0/gg_13_8_otus/blast/0.001:full-ref/table.biom'),\n",
       " ('B1-iter0',\n",
       "  'gg_13_8_otus',\n",
       "  'rdp',\n",
       "  '0.5:full-ref',\n",
       "  '/home/ubuntu/data/short-read-tax-assignment/data/precomputed-results/simulated-community/B1-iter0/gg_13_8_otus/rdp/0.5:full-ref/table.biom'),\n",
       " ('B1-iter0',\n",
       "  'gg_13_8_otus',\n",
       "  'uclust',\n",
       "  '0.51:0.8:1:full-ref',\n",
       "  '/home/ubuntu/data/short-read-tax-assignment/data/precomputed-results/simulated-community/B1-iter0/gg_13_8_otus/uclust/0.51:0.8:1:full-ref/table.biom')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalution 1: Compute and summarize precision, recall, and F-measure for mock communities\n",
    "----------------------------------------------------------------------------------------\n",
    "\n",
    "In this evaluation, we compute and summarize precision, recall, and F-measure of each result (pre-computed and query) based on the known composition of the mock communities. We then summarize the results in two ways: first with boxplots, and second with a table of the top methods based on their F-measures. \n",
    "\n",
    "This is a qualitative evaluation, effectively telling us about the ability of the different methods to report the taxa that are present in each sample. These metrics are not concerned with the abundance of the different taxa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing taxonomic information in table /home/ubuntu/data/short-read-tax-assignment/data/precomputed-results/simulated-community/B1-iter0/gg_13_8_otus/uclust/0.51:0.8:1:full-ref/table.biom, skipping.\n",
      "Missing taxonomic information in table /home/ubuntu/data/short-read-tax-assignment/data/precomputed-results/simulated-community/B1-iter0/gg_13_8_otus/uclust/0.51:0.8:1:full-ref/table.biom, skipping."
     ]
    }
   ],
   "source": [
    "taxa_of_interest = bacterial_reference_taxa\n",
    "\n",
    "taxa_specific_results = []\n",
    "\n",
    "for taxon_of_interest in taxa_of_interest:\n",
    "    try:\n",
    "        query_mock_results = compute_mock_results(results, expected_tables, taxonomy_level=taxonomic_level,\n",
    "                                      min_count=min_count, taxa_to_keep=taxon_of_interest)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    taxa_specific_results.append(query_mock_results)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot infer number of levels from empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-09f63cc395b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtaxon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiIndex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tuples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtaxonomy_level_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtaxonomic_level\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mmetric_by_taxon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethods\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmetric_by_taxon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36mfrom_tuples\u001b[1;34m(cls, tuples, sortorder, names)\u001b[0m\n\u001b[0;32m   3521\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3522\u001b[0m             \u001b[1;31m# I think this is right? Not quite sure...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3523\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot infer number of levels from empty list'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot infer number of levels from empty list"
     ]
    }
   ],
   "source": [
    "indices = []\n",
    "data = []\n",
    "for taxon, result in zip(bacterial_reference_taxa, taxa_specific_results):\n",
    "    values = result['F-measure']\n",
    "    methods = result['Method']\n",
    "    indices.append(list(taxon))\n",
    "    data.append(list(values))\n",
    "index = pd.MultiIndex.from_tuples(indices, names=taxonomy_level_names[:taxonomic_level])\n",
    "metric_by_taxon = pd.DataFrame(np.array(data), columns=list(methods), index=index)\n",
    "metric_by_taxon.sort_index(axis=0, inplace=True)\n",
    "\n",
    "# testing...\n",
    "# metric_by_taxon['rdp']['k__Archaea', 'p__Nanoarchaeota', 'c__[Nanoarchaeoti]'] = 1.0\n",
    "# metric_by_taxon['uclust']['k__Archaea', 'p__Nanoarchaeota', 'c__[Nanoarchaeoti]'] = 1.0\n",
    "# metric_by_taxon['sortmerna']['k__Archaea', 'p__Nanoarchaeota', 'c__[Nanoarchaeoti]'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metric_by_taxon' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-290fc0df8199>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmetric_by_taxon\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'metric_by_taxon' is not defined"
     ]
    }
   ],
   "source": [
    "metric_by_taxon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are method performances correlated? In other words, do all methods do well/bad on the same taxa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print metric_by_taxon.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the performance (F-measure, here) by taxa? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collpase_zero_variance_taxa(df):\n",
    "\n",
    "    index_names = df.index.names\n",
    "    column_names = df.columns\n",
    "    result = df.copy()\n",
    "    for i in range(len(index_names)):\n",
    "        new_indices = []\n",
    "        new_data = []\n",
    "        for d, g in result.groupby(level=range(i+1)):\n",
    "            if len(g) == 1:\n",
    "                new_indices.append(g.index[0])\n",
    "                new_data.append(g.get_values()[0])\n",
    "            elif not g.var().any():\n",
    "                if isinstance(d, tuple):\n",
    "                    n = list(d)\n",
    "                else:\n",
    "                    n = [d]\n",
    "                n += [\"All\"] * (len(index_names) - len(n))\n",
    "                new_indices.append(n)\n",
    "                new_data.append(g.get_values()[0])\n",
    "            else:\n",
    "                new_indices.extend(g.index)\n",
    "                new_data.extend(g.get_values())\n",
    "        new_index = pd.MultiIndex.from_tuples(new_indices, names=index_names)\n",
    "        result = pd.DataFrame(new_data, columns=column_names, index=new_index)\n",
    "    return result\n",
    "\n",
    "metric_by_collapsed_taxon = collpase_zero_variance_taxa(metric_by_taxon)\n",
    "metric_by_collapsed_taxon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouped by taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "height = len(metric_by_collapsed_taxon.index) * 0.35\n",
    "width = len(metric_by_collapsed_taxon.columns) * 1\n",
    "\n",
    "metric_by_collapsed_taxon.sort_index(inplace=True)\n",
    "\n",
    "# Based on SO answer: http://stackoverflow.com/a/18238680\n",
    "fig = plt.figure(figsize=(width, height))\n",
    "heatmap(metric_by_collapsed_taxon, vmin=0, vmax=1, cmap='Reds', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouped by similarity in performance of taxonomic classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#x = metric_by_collapsed_taxon[(metric_by_collapsed_taxon.mean(axis=1) < 1) & (metric_by_collapsed_taxon.mean(axis=1) > -1)]\n",
    "height = len(metric_by_collapsed_taxon.index) * 0.35\n",
    "width = len(metric_by_collapsed_taxon.columns) * 1\n",
    "#fig = plt.figure(figsize=(width, height))\n",
    "clustermap(metric_by_collapsed_taxon, vmin=0, vmax=1, cmap='Reds', figsize=(width, height), col_cluster=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for e in metric_by_collapsed_taxon.index:\n",
    "    print ' '.join(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
