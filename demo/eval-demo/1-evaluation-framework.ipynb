{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Taxonomic assignment method evaluation framework\n",
      "================================================\n",
      "\n",
      "This [IPython Notebook](http://ipython.org/notebook.html) illustrates how to apply te evaluation framework described in (Bokulich, Rideout, et al. (in preparation)). Given a set of precomputed taxonomic assignment method evaluation results (this will likely be the ones included in the [short-read-tax-assignment GitHub repository](https://github.com/gregcaporaso/short-read-tax-assignment/)) and a set of results generated by the user for a new method, the results from the new method are analyzed in the context of the precomputed results. This allows users to rapidly determine how a new method performs, relative to the precomputed results. \n",
      "\n",
      "The following sections present different evaluations with an example usearch-based taxonomic assigner. A parameter sweep was performed using this new taxonomic assigner (as described in the [usearch-tax-assigner notebook]()), and the evaluations that are run here allow us to determine how the new assigner performs relative to the precomputed results and summarize the results in a report, which will be this notebook after all cells have been executed. This provides a convenient interactive framework for analyzing new methods. \n",
      "\n",
      "This notebook could easily be applied to the results of a different taxonomic assigner, and it is therefore intended to serve as a template for report generation. Only one change would need to be made to support that, and that change is discussed inline below.\n",
      "\n",
      "Requirements\n",
      "------------\n",
      "\n",
      "To run this notebook, you'll need to have the following installed:\n",
      "\n",
      " * [IPython](http://ipython.org/)\n",
      " * [PyCogent](https://github.com/pycogent/pycogent)\n",
      " * [short-read-tax-assignment](https://github.com/gregcaporaso/short-read-tax-assignment)\n",
      " * [matplotlib](http://matplotlib.org/)\n",
      " \n",
      "Terminology\n",
      "-----------\n",
      "\n",
      "Through-out the notebook and associated code, we refer to the precomputed results obtained from GitHub as the *subject results*, and the results generated by the user as the *query results*.\n",
      "\n",
      "Structuring query results for comparison to subject results\n",
      "-----------------------------------------------------------\n",
      "\n",
      "To prepare results from another classifier for analysis, you'll need to have [BIOM](http://www.biom-format.org) files with taxonomy assignments as an observation metata category called ``taxonomy``. These should be generated for all analyses (mock community, natural community), datasets, reference databases, methods, and parameter combinations of interest. An example of how to generate these is presented in the [usearch-tax-assigner notebook]()).\n",
      "\n",
      "Your BIOM tables should be called ``table.biom``, and nested in the following directory structure:\n",
      "\n",
      "```\n",
      "query_results_dir/\n",
      " analysis/\n",
      "  dataset-id/ \n",
      "   reference-db-id/\n",
      "    method-id/\n",
      "     parameter-combination-id/\n",
      "      table.biom\n",
      "```\n",
      "\n",
      "``query_results_dir`` is the name of the top level directory, and you will set this value in the first code cell of this notebook. You can name this directory whatever you want to.\n",
      "\n",
      "This directory structure is identical to that for the [subject results](https://github.com/gregcaporaso/short-read-tax-assignment/tree/master/data/eval-subject-results). You can review that directory structure for an example of how this should look.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Configure local environment-specific values\n",
      "-------------------------------------------\n",
      "\n",
      "**This is the only cell that you will need to edit to generate reports locally.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## short_read_tax_dir should be the directory where you've downloaded (or cloned) the \n",
      "## short-read-tax-assignment repository. \n",
      "short_read_tax_dir = \"/Users/caporaso/Dropbox/code/short-read-tax-assignment/\"\n",
      "\n",
      "## query_results_dir should be the directory where the results of the method(s) to be\n",
      "## compared to the pre-computed results can be found. \n",
      "query_results_dir = \"/Users/caporaso/analysis/short-read-tax-eval-prep/10nov2013/\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Prepare the environment\n",
      "-----------------------\n",
      "\n",
      "First we'll configure IPython to add matplotlib plots inline. Then we'll import various functions that we'll need for generating the report. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "from os.path import join, exists\n",
      "from taxcompare.eval_framework import (get_expected_tables_lookup, \n",
      "                                       find_and_process_result_tables,\n",
      "                                       compute_prfs,\n",
      "                                       compute_pearson_spearman,\n",
      "                                       compute_procrustes,\n",
      "                                       generate_pr_scatter_plots,\n",
      "                                       generate_prf_box_plots,\n",
      "                                       generate_precision_box_plots,\n",
      "                                       generate_recall_box_plots,\n",
      "                                       generate_fmeasure_box_plots,\n",
      "                                       generate_prf_table,\n",
      "                                       generate_correlation_box_plots,\n",
      "                                       generate_pearson_box_plots,\n",
      "                                       generate_spearman_box_plots,\n",
      "                                       generate_pearson_spearman_table,\n",
      "                                       generate_procrustes_table)\n",
      "from cogent.draw.distribution_plots import generate_box_plots"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define the subdirectories where the query mock and natural community data should be, and confirm that they exist.\n",
      "mock_query_results_dir = join(query_results_dir,\"mock\")\n",
      "natural_query_results_dir = join(query_results_dir,\"natural\")\n",
      "\n",
      "assert exists(mock_query_results_dir), \"Mock community query directory doesn't exist: %s\" % mock_query_results_dir\n",
      "assert exists(natural_query_results_dir), \"Natural community query directory doesn't exist: %s\" % natural_query_results_dir\n",
      "\n",
      "# Define the subdirectories where the subject mock and natural community data should be, and confirm that they exist.\n",
      "mock_subject_results_dir = join(short_read_tax_dir,'data/eval-pre-computed/mock/')\n",
      "natural_subject_results_dir = join(short_read_tax_dir,'data/eval-pre-computed/natural/')\n",
      "\n",
      "assert exists(mock_query_results_dir), \"Mock community subject directory doesn't exist: %s\" % mock_subject_results_dir\n",
      "assert exists(natural_query_results_dir), \"Natural community subject directory doesn't exist: %s\" % natural_subject_results_dir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Find mock community pre-computed tables, expected tables, and \"query\" tables\n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "Next we'll use the paths defined above to find all of the tables that will be compared. These include the *pre-computed result* tables (i.e., the ones that the new methods will be compared to), the *expected result* tables (i.e., the tables containing the known composition of the mock microbial communities), and the *query result* tables (i.e., the tables generated with the new method(s) that we want to compare to the *pre-computed result* tables)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "query_results = find_and_process_result_tables(mock_query_results_dir)\n",
      "subject_results = find_and_process_result_tables(mock_subject_results_dir)\n",
      "expected_L6_tables = get_expected_tables_lookup(mock_subject_results_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Evalution 1: Compute and summarize precision, recall, and F-measure for mock communities\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "In this evaluation, we compute and summarize precision, recall, and F-measure of each result (pre-computed and query) based on the known composition of the mock communities. We then summarize the results in two ways: first with a scatter plot, where pre-computed results are represented by blue points, and query results are represented by red points; and second with a table of the top twenty-five methods based on their F-measures. \n",
      "\n",
      "This is a qualitative evaluation, effectively telling us about the ability of the different methods to report the taxa that are present in each sample. These metrics are not concerned with the abundance of the different taxa."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subject_prf = list(compute_prfs(subject_results,expected_L6_tables))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "query_prf = list(compute_prfs(query_results,expected_L6_tables))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "generate_pr_scatter_plots(query_prf,subject_prf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "generate_precision_box_plots(query_prf,subject_prf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "generate_recall_box_plots(query_prf,subject_prf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "generate_fmeasure_box_plots(query_prf,subject_prf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "generate_prf_table(query_prf,subject_prf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Evaluation 2: Compute and summarize correlations between observed and known mock community structure\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "In this evaluation, we compute and summarize the correlation between each result (pre-computed and query) and the known composition of the mock communities. We then summarize the results in two ways: first with a series of boxplots of correlation coefficients by method; and second with a table of the top twenty-five methods based on their Spearman correlation coefficient. \n",
      "\n",
      "This is a quantitative evaluation, which tells us about the ability of the different methods to report the taxa that are present in each sample and accurately assess their abundance. Because many factors can affect the observed abundance of taxa beyond the accuracy of the taxonomic assigner (e.g., primer bias), the correlation coefficients are frequently low, but we expect that their relative values are informative in understanding which taxonomic assigners are more correct than others."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subject_pearson_spearman = list(compute_pearson_spearman(subject_results,expected_L6_tables))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "query_pearson_spearman = list(compute_pearson_spearman(query_results,expected_L6_tables))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "generate_pearson_box_plots(query_pearson_spearman,subject_pearson_spearman)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "generate_spearman_box_plots(query_pearson_spearman,subject_pearson_spearman)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "generate_pearson_spearman_table(query_pearson_spearman,subject_pearson_spearman)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Find natural community pre-computed tables, expected tables, and \"query\" tables\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Next we'll use the paths defined above to find all of the natural community tables that will be compared. These include the *pre-computed result* tables (i.e., the ones that the new methods will be compared to), the *expected result* tables (i.e., the OTU table), and the *query result* tables (i.e., the tables generated with the new method(s) that we want to compare to the *pre-computed result* tables)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "query_results = find_and_process_result_tables(natural_query_results_dir)\n",
      "subject_results = find_and_process_result_tables(natural_subject_results_dir)\n",
      "expected_pcs = get_expected_tables_lookup(natural_subject_results_dir,\n",
      "                                          filename_pattern='bray_curtis_pc.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Evaluation 3: Compute and summarize Procrustes analysis results based on real community data\n",
      "--------------------------------------------------------------------------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "query_procrustes = list(compute_procrustes(query_results,expected_pcs))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subject_procrustes = list(compute_procrustes(subject_results,expected_pcs))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "generate_procrustes_table(query_procrustes,subject_procrustes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}